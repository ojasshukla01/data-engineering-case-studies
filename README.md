# ğŸ› ï¸ Data Engineering Case Studies

Welcome to my portfolio repository! This project simulates real-world data engineering workflows using open-source and cloud-native tools â€” from streaming ingestion to analytics-ready models.

It features:
- **Kafka** for real-time data ingestion
- **AWS Lambda-style Python functions** for transformations
- **Snowflake** (or DuckDB locally) for cloud data warehousing
- **DBT** for analytics modeling, testing, and documentation

---

## ğŸš€ Projects Overview

### ğŸ“¡ 1. Kafka â†’ AWS Lambda (Python) â†’ Snowflake Pipeline

This pipeline showcases end-to-end streaming ingestion:
- **Producer** simulates events using [Faker](https://faker.readthedocs.io/)
- **Kafka** (via Docker Compose) handles the streaming layer
- **Lambda-style Python function** transforms events
- **Snowflake/DBT/DuckDB** stores and queries transformed data
- CLI runner script orchestrates the full pipeline

ğŸ“‚ Folder: [`kafka-lambda-snowflake-pipeline/`](./kafka-lambda-snowflake-pipeline)

---

### ğŸ“Š 2. DBT + DuckDB Analytics Stack

This DBT project models raw event data into analytics-friendly marts:
- `stg_events`: staging model
- `fct_actions`: fact model
- Powered by **DBT** with **DuckDB**, no cloud account required

ğŸ“‚ Folder: [`dbt-analytics-stack/`](./dbt-analytics-stack)

---

### ğŸ–¼ï¸ Visual Preview â€“ DBT Docs

Below is a screenshot of the lineage and documentation generated using `dbt docs`:

![DBT Docs Screenshot](assets/dbt_docs_screenshot.png)

---

## ğŸ§± Tech Stack

| Layer         | Tools Used                                      |
|---------------|--------------------------------------------------|
| Ingestion     | Kafka (Docker)                                  |
| Transformation| Python (Lambda-style)                           |
| Orchestration | CLI-based pipeline runner                       |
| Storage       | Snowflake (simulated via DuckDB)                |
| Modeling      | DBT, Jinja2, YAML                               |
| Testing       | dbt seed/run/test/docs                          |

---

## ğŸ§ª How to Run Locally

> Requires: Docker, Python 3.9+, pip, dbt-duckdb

### Kafka + Event Producer

```bash
cd kafka-lambda-snowflake-pipeline
docker-compose up        # Start Kafka and Zookeeper
python producer/produce_events.py
```

### Lambda Function + DB Loader

```bash
python run_pipeline.py   # Listens to Kafka â†’ Transforms â†’ Loads into DuckDB
```

### DBT Transformation (staging + marts)

```bash
cd dbt-analytics-stack
dbt seed --profiles-dir templates/
dbt run --profiles-dir templates/
```

### DBT Docs UI

```bash
dbt docs generate --profiles-dir templates/
dbt docs serve --profiles-dir templates/
```

---

## ğŸ“˜ Full Walkthrough

ğŸ“– See [`RUN_GUIDE.md`](./RUN_GUIDE.md) for the complete step-by-step instructions to run this project from end to end.

---

## ğŸ‘¤ About Me

Hi, I'm **Ojas Shukla** â€” a Data Engineer passionate about building efficient, scalable data pipelines that solve real-world problems.

With 4+ years of experience in cloud-native data platforms, analytics engineering, and ETL systems, I specialize in:
- Real-time + batch processing
- GCP & AWS-based data stacks
- dbt, DuckDB, Python, SQL, Kafka

ğŸ“« [Connect with me on LinkedIn](https://www.linkedin.com/in/ojasshukla01)

ğŸ§° [Explore other projects â†’](https://github.com/ojasshukla01)

---

Built with â¤ï¸ to showcase the real-world data flow from stream ingestion to insight.

# â–¶ï¸ RUN GUIDE: Kafka â†’ Python â†’ DuckDB â†’ DBT Pipeline

This guide walks you through running the full real-time data pipeline simulation.

---

## ğŸ§° Prerequisites

- Docker Desktop (with WSL2/Linux containers enabled)
- Python 3.10+
- `pip` and `virtualenv` (optional)
- Git (to clone the repo)

---

## ğŸ“¦ Project Structure

```
data-engineering-case-studies/
â”œâ”€â”€ kafka-lambda-snowflake-pipeline/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ producer/produce_events.py
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ snowflake_loader/load.py
â”‚   â””â”€â”€ events.duckdb
â”œâ”€â”€ dbt-analytics-stack/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ data/events.csv
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ templates/profiles.yml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .venv/
â”œâ”€â”€ RUN_GUIDE.md
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Step 1: Set Up the Environment

1. Create a virtual environment:
   ```bash
   python -m venv .venv
   .\.venv\Scripts\Activate.ps1
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

> Don't have a `requirements.txt`? Run `pip freeze > requirements.txt` once everything works.

---

## ğŸ³ Step 2: Start Kafka and Zookeeper

In PowerShell or Terminal:

```bash
cd kafka-lambda-snowflake-pipeline
docker-compose up
```

âœ… **Expected output:** Kafka and Zookeeper logs. Look for:
```
KafkaServer id=1 started
```

ğŸ“Œ Leave this terminal open while running the rest.

---

## ğŸ“¡ Step 3: Start Producing Events (Simulated)

In a new terminal window:

```bash
cd kafka-lambda-snowflake-pipeline
.\.venv\Scripts\Activate.ps1
python producer/produce_events.py
```

âœ… **Expected output:**
```
Sent: {'user_id': ..., 'event_type': 'click', 'timestamp': ...}
```

ğŸ“Œ This runs **forever**. Stop it anytime using `Ctrl + C`.

---

## ğŸ”„ Step 4: Start the Pipeline Listener

In another terminal:

```bash
cd kafka-lambda-snowflake-pipeline
.\.venv\Scripts\Activate.ps1
python run_pipeline.py
```

âœ… **Expected output:**
```
Received: ...
Transformed: ...
âœ… Inserted into DuckDB: ...
```

ğŸ“Œ This also runs indefinitely â€” `Ctrl + C` to stop.

---

## ğŸ§ª Step 5: Query the Local Database (Optional)

Check if events are being stored:

```bash
python query_duckdb.py
```

âœ… **Expected output:**
```text
                                  id   action         event_time
0  a1b2c3d4-...                   click  2025-05-10T...
```

---

## ğŸ“Š Step 6: Run DBT Analytics Models

In a new terminal:

```bash
cd dbt-analytics-stack
.\.venv\Scripts\Activate.ps1

# Run analytics
dbt debug --profiles-dir templates/
dbt seed --profiles-dir templates/
dbt run --profiles-dir templates/
```

âœ… **Expected output:**
```
Finished running 2 models
```

---

## ğŸ“– Step 7: Generate and View DBT Documentation

```bash
dbt docs generate --profiles-dir templates/
dbt docs serve --profiles-dir templates/
```

Open [http://localhost:8080](http://localhost:8080) in your browser.

âœ… Explore:
- Model lineage
- SQL preview
- Column-level descriptions

---

## ğŸ§¼ Step 8: Shut Down Everything

When done:

1. Stop all running Python scripts with `Ctrl + C`
2. Stop Docker:
   ```bash
   docker-compose down
   ```

---

## âœ… Summary

| Step                    | Description                           | Runs Until       |
|-------------------------|----------------------------------------|------------------|
| Docker Compose          | Starts Kafka & Zookeeper               | Manual Ctrl+C    |
| `produce_events.py`     | Sends fake events                      | Manual Ctrl+C    |
| `run_pipeline.py`       | Transforms & loads to DuckDB           | Manual Ctrl+C    |
| `query_duckdb.py`       | Validates data landed                  | One-time query   |
| DBT + Docs              | Models & visualizes event data         | `Ctrl+C` to exit |

---

## ğŸ§  Pro Tips

- All data is stored locally in `events.duckdb`
- You can reset the DB by deleting that file
- `query_duckdb.py` is your best friend for debugging
- The system simulates **real-time pipelines** in a fully local setup

---

Built by [Ojas Shukla](https://www.linkedin.com/in/ojasshukla01)
